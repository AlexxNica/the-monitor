# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2016, Datadog
<<<<<<< HEAD
# This file is distributed under the same license as the the monitor package.
=======
# This file is distributed under the same license as the The monitor package.
>>>>>>> sphinx-po
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
<<<<<<< HEAD
"Project-Id-Version: the monitor 0.1\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2016-04-18 21:28+0900\n"
=======
"Project-Id-Version: The monitor 0.1\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2016-04-19 15:39+0900\n"
>>>>>>> sphinx-po
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../source/monitoring-101/index.rst:2
<<<<<<< HEAD
msgid "基本編"
=======
msgid "monitoring 101"
>>>>>>> sphinx-po
msgstr ""

#: ../../source/monitoring-101/index.rst:4
msgid "Contents:"
msgstr ""

#: ../../source/monitoring-101/monitoring_101_alerting_on_what_matters.md:0
msgid "Monitoring 101: Alerting on what matters"
msgstr ""

#: ../../source/monitoring-101/monitoring_101_alerting_on_what_matters.md:4
msgid "This post is part of a series on effective monitoring. Be sure to check out the rest of the series: Collecting the right data and Investigating performance issues."
msgstr ""

#: ../../source/monitoring-101/monitoring_101_alerting_on_what_matters.md:7
msgid "Automated alerts are essential to monitoring. They allow you to spot problems anywhere in your infrastructure, so that you can rapidly identify their causes and minimize service degradation and disruption."
msgstr ""

#: ../../source/monitoring-101/monitoring_101_alerting_on_what_matters.md:11
msgid "But alerts aren’t always as effective as they could be. In particular, real problems are often lost in a sea of noisy alarms. This article describes a simple approach to effective alerting, regardless of the scale of the systems involved. In short:"
msgstr ""

#: ../../source/monitoring-101/monitoring_101_alerting_on_what_matters.md:19
#: ../../source/monitoring-101/monitoring_101_collecting_the_right_data.md:22
#: ../../source/monitoring-101/monitoring_101_investigating_performance_issues.md:7
msgid "This series of articles comes out of our experience monitoring large-scale infrastructure for our customers. It also draws on the work of Brendan Gregg, Rob Ewaschuk, and Baron Schwartz."
msgstr ""

#: ../../source/monitoring-101/monitoring_101_alerting_on_what_matters.md:0
msgid "When to alert someone (or no one)"
msgstr ""

#: ../../source/monitoring-101/monitoring_101_alerting_on_what_matters.md:31
msgid "An alert should communicate something specific about your systems in plain language: “Two Cassandra nodes are down” or “90% of all web requests are taking more than 0.5s to process and respond.” Automating alerts across as many of your systems as possible allows you to respond quickly to issues and provide better service, and it also saves time by freeing you from continual manual inspection of metrics."
msgstr ""

#: ../../source/monitoring-101/monitoring_101_alerting_on_what_matters.md:0
msgid "Levels of alerting urgency"
msgstr ""

#: ../../source/monitoring-101/monitoring_101_alerting_on_what_matters.md:40
msgid "Not all alerts carry the same degree of urgency. Some require immediate human intervention, some require eventual human intervention, and some point to areas where attention may be needed in the future. All alerts should, at a minimum, be logged to a central location for easy correlation with other metrics and events."
msgstr ""

#: ../../source/monitoring-101/monitoring_101_alerting_on_what_matters.md:0
msgid "Alerts as records (low severity)"
msgstr ""

#: ../../source/monitoring-101/monitoring_101_alerting_on_what_matters.md:48
msgid "Many alerts will not be associated with a service problem, so a human may never even need to be aware of them. For instance, when a data store that supports a user-facing service starts serving queries much slower than usual, but not slow enough to make an appreciable difference in the overall service’s response time, that should generate a low-urgency alert that is recorded in your monitoring system for future reference or investigation but does not interrupt anyone’s work. After all, transient issues that could be to blame, such as network congestion, often go away on their own. But should a significant issue develop—say, if the service starts returning a large number of timeouts—that alert-based data will provide invaluable context for your investigation."
msgstr ""

#: ../../source/monitoring-101/monitoring_101_alerting_on_what_matters.md:0
msgid "Alerts as notifications (moderate severity)"
msgstr ""

#: ../../source/monitoring-101/monitoring_101_alerting_on_what_matters.md:62
msgid "The next tier of alerting urgency is for issues that do require intervention, but not right away. Perhaps the data store is running low on disk space and should be scaled out in the next several days. Sending an email and/or posting a notification in the service owner’s chat room is a perfect way to deliver these alerts—both message types are highly visible, but they won’t wake anyone in the middle of the night or disrupt an engineer’s flow."
msgstr ""

#: ../../source/monitoring-101/monitoring_101_alerting_on_what_matters.md:0
msgid "Alerts as pages (high severity)"
msgstr ""

#: ../../source/monitoring-101/monitoring_101_alerting_on_what_matters.md:72
msgid "The most urgent alerts should receive special treatment and be escalated to a page (as in “pager”) to urgently request human attention. Response times for your web application, for instance, should have an internal SLA that is at least as aggressive as your strictest customer-facing SLA. Any instance of response times exceeding your internal SLA would warrant immediate attention, whatever the hour."
msgstr ""

#: ../../source/monitoring-101/monitoring_101_alerting_on_what_matters.md:0
msgid "When to let a sleeping engineer lie"
msgstr ""

#: ../../source/monitoring-101/monitoring_101_alerting_on_what_matters.md:84
msgid "Whenever you consider setting an alert, ask yourself three questions to determine the alert’s level of urgency and how it should be handled:"
msgstr ""

#: ../../source/monitoring-101/monitoring_101_alerting_on_what_matters.md:0
msgid "Page on symptoms"
msgstr ""

#: ../../source/monitoring-101/monitoring_101_alerting_on_what_matters.md:124
msgid "Pages deserve special mention: they are extremely effective for delivering information, but they can be quite disruptive if overused, or if they are linked to poorly designed alerts. In general, a page is the most appropriate kind of alert when the system you are responsible for stops doing useful work with acceptable throughput, latency, or error rates. Those are the sort of problems that you want to know about immediately."
msgstr ""

#: ../../source/monitoring-101/monitoring_101_alerting_on_what_matters.md:132
msgid "The fact that your system stopped doing useful work is a symptom—that is, it is a manifestation of an issue that may have any number of different causes. For example: if your website has been responding very slowly for the last three minutes, that is a symptom. Possible causes include high database latency, failed application servers, Memcached being down, high load, and so on. Whenever possible, build your pages around symptoms rather than causes. See our companion article on data collection for a metric framework that helps to separate symptoms from causes."
msgstr ""

#: ../../source/monitoring-101/monitoring_101_alerting_on_what_matters.md:142
msgid "Paging on symptoms surfaces real, oftentimes user-facing problems, rather than hypothetical or internal problems. Contrast paging on a symptom, such as slow website responses, with paging on potential causes of the symptom, such as high load on your web servers. Your users will not know or care about server load if the website is still responding quickly, and your engineers will resent being bothered for something that is only internally noticeable and that may revert to normal levels without intervention."
msgstr ""

#: ../../source/monitoring-101/monitoring_101_alerting_on_what_matters.md:0
msgid "Durable alert definitions"
msgstr ""

#: ../../source/monitoring-101/monitoring_101_alerting_on_what_matters.md:153
msgid "Another good reason to page on symptoms is that symptom-triggered alerts tend to be durable. This means that regardless of how underlying system architectures may change, if the system stops doing work as well as it should, you will get an appropriate page even without updating your alert definitions."
msgstr ""

#: ../../source/monitoring-101/monitoring_101_alerting_on_what_matters.md:0
msgid "Exception to the rule: Early warning signs"
msgstr ""

#: ../../source/monitoring-101/monitoring_101_alerting_on_what_matters.md:163
msgid "It is sometimes necessary to call human attention to a small handful of metrics even when the system is performing adequately. Early warning metrics reflect an unacceptably high probability that serious symptoms will soon develop and require immediate intervention."
msgstr ""

#: ../../source/monitoring-101/monitoring_101_alerting_on_what_matters.md:168
msgid "Disk space is a classic example. Unlike running out of free memory or CPU, when you run out of disk space, the system will not likely recover, and you probably will have only a few seconds before your system hard stops. Of course, if you can notify someone with plenty of lead time, then there is no need to wake anyone in the middle of the night. Better yet, you can anticipate some situations when disk space will run low and build automated remediation based on the data you can afford to erase, such as logs or data that exists somewhere else."
msgstr ""

#: ../../source/monitoring-101/monitoring_101_alerting_on_what_matters.md:0
msgid "Conclusion: Get serious about symptoms"
msgstr ""

#: ../../source/monitoring-101/monitoring_101_alerting_on_what_matters.md:187
#: ../../source/monitoring-101/monitoring_101_collecting_the_right_data.md:233
#: ../../source/monitoring-101/monitoring_101_investigating_performance_issues.md:63
msgid "We would like to hear about your experiences as you apply this framework to your own monitoring practice. If it is working well, please let us know on Twitter! Questions, corrections, additions, complaints, etc? Please let us know on GitHub."
msgstr ""

#: ../../source/monitoring-101/monitoring_101_collecting_the_right_data.md:0
msgid "Monitoring 101: Collecting the right data"
msgstr ""

#: ../../source/monitoring-101/monitoring_101_collecting_the_right_data.md:4
msgid "This post is part of a series on effective monitoring. Be sure to check out the rest of the series: Alerting on what matters and Investigating performance issues."
msgstr ""

#: ../../source/monitoring-101/monitoring_101_collecting_the_right_data.md:7
msgid "Monitoring data comes in a variety of forms—some systems pour out data continuously and others only produce data when rare events occur. Some data is most useful for identifying problems; some is primarily valuable for investigating problems. This post covers which data to collect, and how to classify that data so that you can:"
msgstr ""

#: ../../source/monitoring-101/monitoring_101_collecting_the_right_data.md:16
msgid "Whatever form your monitoring data takes, the unifying theme is this:"
msgstr ""

#: ../../source/monitoring-101/monitoring_101_collecting_the_right_data.md:0
msgid "Metrics"
msgstr ""

#: ../../source/monitoring-101/monitoring_101_collecting_the_right_data.md:33
msgid "Metrics capture a value pertaining to your systems at a specific point in time—for example, the number of users currently logged in to a web application. Therefore, metrics are usually collected once per second, one per minute, or at another regular interval to monitor a system over time. There are two important categories of metrics in our framework: work metrics and resource metrics. For each system that is part of your software infrastructure, consider which work metrics and resource metrics are reasonably available, and collect them all."
msgstr ""

#: ../../source/monitoring-101/monitoring_101_collecting_the_right_data.md:0
msgid "Work metrics"
msgstr ""

#: ../../source/monitoring-101/monitoring_101_collecting_the_right_data.md:46
msgid "Work metrics indicate the top-level health of your system by measuring its useful output. When considering your work metrics, it’s often helpful to break them down into four subtypes:"
msgstr ""

#: ../../source/monitoring-101/monitoring_101_collecting_the_right_data.md:66
msgid "Below are example work metrics of all four subtypes for two common kinds of systems: a web server and a data store."
msgstr ""

#: ../../source/monitoring-101/monitoring_101_collecting_the_right_data.md:69
msgid "Example work metrics: Web server (at time 2015-04-24 08:13:01 UTC)"
msgstr ""

#: ../../source/monitoring-101/monitoring_101_collecting_the_right_data.md:71
msgid "| Subtype | Description                                              | Value | |-------------|--------------------------------------------------------------|-----------| | throughput  |  requests per second                                         | 312       | | success     |  percentage of responses that are 2xx since last measurement | 99.1      | | error       |  percentage of responses that are 5xx since last measurement | 0.1       | | latency     |  90th percentile response time in seconds                    | 0.4       |"
msgstr ""

#: ../../source/monitoring-101/monitoring_101_collecting_the_right_data.md:78
msgid "Example work metrics: Data store (at time 2015-04-24 08:13:01 UTC)"
msgstr ""

#: ../../source/monitoring-101/monitoring_101_collecting_the_right_data.md:80
msgid "| Subtype | Description                                                    | Value | |-------------|--------------------------------------------------------------------|-----------| | throughput  | queries per second                                                 | 949       | | success     | percentage of queries successfully executed since last measurement | 100       | | error       | percentage of queries yielding exceptions since last measurement   | 0         | | error       | percentage of queries returning stale data since last measurement  | 4.2       | | latency     | 90th percentile query time in seconds                              | 0.02      |"
msgstr ""

#: ../../source/monitoring-101/monitoring_101_collecting_the_right_data.md:0
msgid "Resource metrics"
msgstr ""

#: ../../source/monitoring-101/monitoring_101_collecting_the_right_data.md:90
msgid "Most components of your software infrastructure serve as a resource to other systems. Some resources are low-level—for instance, a server’s resources include such physical components as CPU, memory, disks, and network interfaces. But a higher-level component, such as a database or a geolocation microservice, can also be considered a resource if another system requires that component to produce work."
msgstr ""

#: ../../source/monitoring-101/monitoring_101_collecting_the_right_data.md:97
msgid "Resource metrics are especially valuable for investigation and diagnosis of problems. For each resource in your system, try to collect metrics that cover four key areas:"
msgstr ""

#: ../../source/monitoring-101/monitoring_101_collecting_the_right_data.md:112
msgid "Here are example metrics for a handful of common resource types:"
msgstr ""

#: ../../source/monitoring-101/monitoring_101_collecting_the_right_data.md:114
msgid "| Resource | Utilization                                       | Saturation       | Errors                                   | Availability             ||--------------|-------------------------------------------------------|----------------------|----------------------------------------------|------------------------------| | Disk IO      | % time that device was busy                           | wait queue length    | # device errors                             | % time writable              || Memory       | % of total memory capacity in use                     | swap usage           | N/A (not usually observable)                 | N/A                          || Microservice | average % time each request-servicing thread was busy | # enqueued requests | # internal errors such as caught exceptions | % time service is reachable  || Database     | average % time each connection was busy               | # enqueued queries  | # internal errors, e.g. replication errors  | % time database is reachable |"
msgstr ""

#: ../../source/monitoring-101/monitoring_101_collecting_the_right_data.md:0
msgid "Other metrics"
msgstr ""

#: ../../source/monitoring-101/monitoring_101_collecting_the_right_data.md:123
msgid "There are a few other types of metrics that are neither work nor resource metrics, but that nonetheless may come in handy in diagnosing causes of problems. Common examples include counts of cache hits or database locks. When in doubt, capture the data."
msgstr ""

#: ../../source/monitoring-101/monitoring_101_collecting_the_right_data.md:0
msgid "Events"
msgstr ""

#: ../../source/monitoring-101/monitoring_101_collecting_the_right_data.md:130
msgid "In addition to metrics, which are collected more or less continuously, some monitoring systems can also capture events: discrete, infrequent occurrences that can provide crucial context for understanding what changed in your system’s behavior. Some examples:"
msgstr ""

#: ../../source/monitoring-101/monitoring_101_collecting_the_right_data.md:139
msgid "An event usually carries enough information that it can be interpreted on its own, unlike a single metric data point, which is generally only meaningful in context. Events capture what happened, at a point in time, with optional additional information. For example:"
msgstr ""

#: ../../source/monitoring-101/monitoring_101_collecting_the_right_data.md:144
msgid "| What happened                     | Time                | Additional information | |---------------------------------------|-------------------------|----------------------------| | Hotfix f464bfe released to production | 2015–05–15 04:13:25 UTC | Time elapsed: 1.2 seconds  | | Pull request 1630 merged              | 2015–05–19 14:22:20 UTC | Commits: ea720d6           | | Nightly data rollup failed            | 2015–05–27 00:03:18 UTC | Link to logs of failed job |"
msgstr ""

#: ../../source/monitoring-101/monitoring_101_collecting_the_right_data.md:150
msgid "Events are sometimes used used to generate alerts—someone should be notified of events such as the third example in the table above, which indicates that critical work has failed. But more often they are used to investigate issues and correlate across systems. In general, think of events like metrics—they are valuable data to be collected wherever it is feasible."
msgstr ""

#: ../../source/monitoring-101/monitoring_101_collecting_the_right_data.md:0
msgid "What good data looks like"
msgstr ""

#: ../../source/monitoring-101/monitoring_101_collecting_the_right_data.md:161
msgid "The data you collect should have four characteristics:"
msgstr ""

#: ../../source/monitoring-101/monitoring_101_collecting_the_right_data.md:0
msgid "Data for alerts and diagnostics"
msgstr ""

#: ../../source/monitoring-101/monitoring_101_collecting_the_right_data.md:196
msgid "The table below maps the different data types described in this article to different levels of alerting urgency outlined in a companion post. In short, a record is a low-urgency alert that does not notify anyone automatically but is recorded in a monitoring system in case it becomes useful for later analysis or investigation. A notification is a moderate-urgency alert that notifies someone who can fix the problem in a non-interrupting way such as email or chat. A page is an urgent alert that interrupts a recipient’s work, sleep, or personal time, whatever the hour. Note that depending on severity, a notification may be more appropriate than a page, or vice versa:"
msgstr ""

#: ../../source/monitoring-101/monitoring_101_collecting_the_right_data.md:207
msgid "| Data                      | Alert    | Trigger                                                                         | |-------------------------------|--------------|-------------------------------------------------------------------------------------| | Work metric: Throughput       | Page         | value is much higher or lower than usual, or there is an anomalous rate of change   || Work metric: Success          | Page         | the percentage of work that is successfully processed drops below a threshold       || Work metric: Errors           | Page         | the error rate exceeds a threshold                                                  || Work metric: Performance      | Page         | work takes too long to complete (e.g., performance violates internal SLA)           || Resource metric: Utilization  | Notification | approaching critical resource limit (e.g., free disk space drops below a threshold) || Resource metric: Saturation   | Record       | number of waiting processes exceeds a threshold                                     || Resource metric: Errors       | Record       | number of errors during a fixed period exceeds a threshold                          || Resource metric: Availability | Record       | the resource is unavailable for a percentage of time that exceeds a threshold       || Event: Work-related           | Page         | critical work that should have been completed is reported as incomplete or failed   |"
msgstr ""

#: ../../source/monitoring-101/monitoring_101_collecting_the_right_data.md:0
msgid "Conclusion: Collect ’em all"
msgstr ""

#: ../../source/monitoring-101/monitoring_101_investigating_performance_issues.md:0
msgid "Monitoring 101: Investigating performance issues"
msgstr ""

#: ../../source/monitoring-101/monitoring_101_investigating_performance_issues.md:3
msgid "This post is part of a series on effective monitoring. Be sure to check out the rest of the series: Collecting the right data and Alerting on what matters."
msgstr ""

#: ../../source/monitoring-101/monitoring_101_investigating_performance_issues.md:5
msgid "The responsibilities of a monitoring system do not end with symptom detection. Once your monitoring system has notified you of a real symptom that requires attention, its next job is to help you diagnose the root cause. Often this is the least structured aspect of monitoring, driven largely by hunches and guess-and-check. This post describes a more directed approach that can help you to find and correct root causes more efficiently."
msgstr ""

#: ../../source/monitoring-101/monitoring_101_investigating_performance_issues.md:0
msgid "A word about data"
msgstr ""

#: ../../source/monitoring-101/monitoring_101_investigating_performance_issues.md:11
msgid "metric types"
msgstr ""

#: ../../source/monitoring-101/monitoring_101_investigating_performance_issues.md:13
msgid "There are three main types of monitoring data that can help you investigate the root causes of problems in your infrastructure. Data types and best practices for their collection are discussed in-depth in a companion post, but in short:"
msgstr ""

#: ../../source/monitoring-101/monitoring_101_investigating_performance_issues.md:19
msgid "By and large, work metrics will surface the most serious symptoms and should therefore generate the most serious alerts. But the other metric types are invaluable for investigating the causes of those symptoms."
msgstr ""

#: ../../source/monitoring-101/monitoring_101_investigating_performance_issues.md:0
msgid "It’s resources all the way down"
msgstr ""

#: ../../source/monitoring-101/monitoring_101_investigating_performance_issues.md:23
msgid "metric uses"
msgstr ""

#: ../../source/monitoring-101/monitoring_101_investigating_performance_issues.md:25
msgid "Most of the components of your infrastructure can be thought of as resources. At the highest levels, each of your systems that produces useful work likely relies on other systems. For instance, the Apache server in a LAMP stack relies on a MySQL database as a resource to support its work. One level down, within MySQL are database-specific resources that MySQL uses to do its work, such as the finite pool of client connections. At a lower level still are the physical resources of the server running MySQL, such as CPU, memory, and disks."
msgstr ""

#: ../../source/monitoring-101/monitoring_101_investigating_performance_issues.md:27
msgid "Thinking about which systems produce useful work, and which resources support that work, can help you to efficiently get to the root of any issues that surface. When an alert notifies you of a possible problem, the following process will help you to approach your investigation systematically."
msgstr ""

#: ../../source/monitoring-101/monitoring_101_investigating_performance_issues.md:29
msgid "recursive investigation"
msgstr ""

#: ../../source/monitoring-101/monitoring_101_investigating_performance_issues.md:0
msgid "1. Start at the top with work metrics"
msgstr ""

#: ../../source/monitoring-101/monitoring_101_investigating_performance_issues.md:33
msgid "First ask yourself, “Is there a problem? How can I characterize it?” If you don’t describe the issue clearly at the outset, it’s easy to lose track as you dive deeper into your systems to diagnose the issue."
msgstr ""

#: ../../source/monitoring-101/monitoring_101_investigating_performance_issues.md:35
msgid "Next examine the work metrics for the highest-level system that is exhibiting problems. These metrics will often point to the source of the problem, or at least set the direction for your investigation. For example, if the percentage of work that is successfully processed drops below a set threshold, diving into error metrics, and especially the types of errors being returned, will often help narrow the focus of your investigation. Alternatively, if latency is high, and the throughput of work being requested by outside systems is also very high, perhaps the system is simply overburdened."
msgstr ""

#: ../../source/monitoring-101/monitoring_101_investigating_performance_issues.md:0
msgid "2. Dig into resources"
msgstr ""

#: ../../source/monitoring-101/monitoring_101_investigating_performance_issues.md:39
msgid "If you haven’t found the cause of the problem by inspecting top-level work metrics, next examine the resources that the system uses—physical resources as well as software or external services that serve as resources to the system. If you’ve already set up dashboards for each system as outlined below, you should be able to quickly find and peruse metrics for the relevant resources. Are those resources unavailable? Are they highly utilized or saturated? If so, recurse into those resources and begin investigating each of them at step 1."
msgstr ""

#: ../../source/monitoring-101/monitoring_101_investigating_performance_issues.md:0
msgid "3. Did something change?"
msgstr ""

#: ../../source/monitoring-101/monitoring_101_investigating_performance_issues.md:43
msgid "Next consider alerts and other events that may be correlated with your metrics. If a code release, internal alert, or other event was registered slightly before problems started occurring, investigate whether they may be connected to the problem."
msgstr ""

#: ../../source/monitoring-101/monitoring_101_investigating_performance_issues.md:0
msgid "4. Fix it (and don’t forget it)"
msgstr ""

#: ../../source/monitoring-101/monitoring_101_investigating_performance_issues.md:47
msgid "Once you have determined what caused the issue, correct it. Your investigation is complete when symptoms disappear—you can now think about how to change the system to avoid similar problems in the future."
msgstr ""

#: ../../source/monitoring-101/monitoring_101_investigating_performance_issues.md:0
msgid "Build dashboards before you need them"
msgstr ""

#: ../../source/monitoring-101/monitoring_101_investigating_performance_issues.md:51
msgid "dashboard"
msgstr ""

#: ../../source/monitoring-101/monitoring_101_investigating_performance_issues.md:53
msgid "In an outage, every minute is crucial. To speed your investigation and keep your focus on the task at hand, set up dashboards in advance. You may want to set up one dashboard for your high-level application metrics, and one dashboard for each subsystem. Each system’s dashboard should render the work metrics of that system, along with resource metrics of the system itself and key metrics of the subsystems it depends on. If event data is available, overlay relevant events on the graphs for correlation analysis."
msgstr ""

#: ../../source/monitoring-101/monitoring_101_investigating_performance_issues.md:0
msgid "Conclusion: Follow the metrics"
msgstr ""

#: ../../source/monitoring-101/monitoring_101_investigating_performance_issues.md:57
msgid "Adhering to a standardized monitoring framework allows you to investigate problems more systematically:"
msgstr ""

